# Language Modeling

<!-- Semantic embeddings, such as Word2Vec and GloVe, are in fact a first step towards **language modeling** - creating models that somehow *understand* (or *represent*) the nature of the language. -->
è¯­ä¹‰åµŒå…¥ï¼Œä¾‹å¦‚ Word2Vec å’Œ GloVeï¼Œå®é™…ä¸Šæ˜¯**è¯­è¨€å»ºæ¨¡**çš„ç¬¬ä¸€æ­¥- åˆ›å»ºä»¥æŸç§æ–¹å¼*ç†è§£*ï¼ˆæˆ–*è¡¨ç¤º*ï¼‰è¯­è¨€æœ¬è´¨çš„æ¨¡å‹ã€‚

## [Pre-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/115)

<!-- The main idea behind language modeling is training them on unlabeled datasets in an unsupervised manner. This is important because we have huge amounts of unlabeled text available, while the amount of labeled text would always be limited by the amount of effort we can spend on labeling. Most often, we can build language models that can **predict missing words** in the text, because it is easy to mask out a random word in text and use it as a training sample. -->
è¯­è¨€å»ºæ¨¡èƒŒåçš„ä¸»è¦æ€æƒ³æ˜¯ä»¥æ— ç›‘ç£çš„æ–¹å¼åœ¨æœªæ ‡è®°çš„æ•°æ®é›†ä¸Šè®­ç»ƒå®ƒä»¬ã€‚è¿™å¾ˆé‡è¦ï¼Œå› ä¸ºæˆ‘ä»¬æœ‰å¤§é‡æœªæ ‡è®°çš„æ–‡æœ¬å¯ç”¨ï¼Œè€Œæ ‡è®°æ–‡æœ¬çš„æ•°é‡å§‹ç»ˆå—åˆ°æˆ‘ä»¬åœ¨æ ‡è®°ä¸ŠèŠ±è´¹çš„ç²¾åŠ›çš„é™åˆ¶ã€‚å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥æ„å»ºå¯ä»¥**é¢„æµ‹æ–‡æœ¬ä¸­ç¼ºå¤±å•è¯**çš„è¯­è¨€æ¨¡å‹ï¼Œå› ä¸ºå¾ˆå®¹æ˜“å±è”½æ–‡æœ¬ä¸­çš„éšæœºå•è¯å¹¶å°†å…¶ç”¨ä½œè®­ç»ƒæ ·æœ¬ã€‚

## Training Embeddings

<!-- In our previous examples, we used pre-trained semantic embeddings, but it is interesting to see how those embeddings can be trained. There are several possible ideas the can be used: -->
åœ¨ä¹‹å‰çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†é¢„å…ˆè®­ç»ƒçš„è¯­ä¹‰åµŒå…¥ï¼Œä½†äº†è§£å¦‚ä½•è®­ç»ƒè¿™äº›åµŒå…¥å¾ˆæœ‰è¶£ã€‚æœ‰å‡ ç§å¯èƒ½çš„æƒ³æ³•å¯ä»¥ä½¿ç”¨ï¼š

<!-- * **N-Gram** language modeling, when we predict a token by looking at N previous tokens (N-gram)
* **Continuous Bag-of-Words** (CBoW), when we predict the middle token $W_0$ in a token sequence $W_{-N}$, ..., $W_N$.
* **Skip-gram**, where we predict a set of neighboring tokens {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$} from the middle token $W_0$. -->

* **N-Gram** N-Gramè¯­è¨€å»ºæ¨¡ï¼Œå½“æˆ‘ä»¬é€šè¿‡æŸ¥çœ‹ N ä¸ªå…ˆå‰çš„æ ‡è®°æ¥é¢„æµ‹æ ‡è®°æ—¶ (N-gram)
* **Continuous Bag-of-Words** (CBoW)è¿ç»­è¯è¢‹ï¼ˆCBoWï¼‰ï¼Œå½“æˆ‘ä»¬é¢„æµ‹æ ‡è®°åºåˆ— $W_{-N}$, ..., $W_N$ ä¸­çš„ä¸­é—´æ ‡è®° $W_0$ æ—¶ã€‚
* **Skip-gram**ï¼Œæˆ‘ä»¬ä»ä¸­é—´æ ‡è®° $W_0$ é¢„æµ‹ä¸€ç»„ç›¸é‚»æ ‡è®° {$W_{-N},\dots, W_{-1}, W_1,\dots, W_N$}ã€‚

![image from paper on converting words to vectors](../14-Embeddings/images/example-algorithms-for-converting-words-to-vectors.png)

> Image from [this paper](https://arxiv.org/pdf/1301.3781.pdf)

## âœï¸ Example Notebooks: Training CBoW model

Continue your learning in the following notebooks:

* [Training CBoW Word2Vec with TensorFlow](CBoW-TF.ipynb)
* [Training CBoW Word2Vec with PyTorch](CBoW-PyTorch.ipynb)


## Conclusion

<!-- In the previous lesson we have seen that words embeddings work like magic! Now we know that training word embeddings is not a very complex task, and we should be able to train our own word embeddings for domain specific text if needed.  -->
åœ¨ä¸Šä¸€è¯¾ä¸­ï¼Œæˆ‘ä»¬å·²ç»çœ‹åˆ°å•è¯åµŒå…¥å°±åƒé­”æ³•ä¸€æ ·ï¼ç°åœ¨æˆ‘ä»¬çŸ¥é“è®­ç»ƒè¯åµŒå…¥å¹¶ä¸æ˜¯ä¸€é¡¹éå¸¸å¤æ‚çš„ä»»åŠ¡ï¼Œå¦‚æœéœ€è¦ï¼Œæˆ‘ä»¬åº”è¯¥èƒ½å¤Ÿé’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„æ–‡æœ¬è®­ç»ƒæˆ‘ä»¬è‡ªå·±çš„è¯åµŒå…¥ã€‚

## [Post-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/215)

## Review & Self Study

* [Official PyTorch tutorial on Language Modeling](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).
* [Official TensorFlow tutorial on training Word2Vec model](https://www.TensorFlow.org/tutorials/text/word2vec).
* Using the **gensim** framework to train most commonly used embeddings in a few lines of code is described [in this documentation](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).

## ğŸš€ [Assignment: Train Skip-Gram Model](lab/README.md)

In the lab, we challenge you to modify the code from this lesson to train skip-gram model instead of CBoW. [Read the details](lab/README.md)
