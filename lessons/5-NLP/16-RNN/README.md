# Recurrent Neural Networks

## [Pre-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/116)

<!-- In previous sections, we have been using rich semantic representations of text and a simple linear classifier on top of the embeddings. What this architecture does is to capture the aggregated meaning of words in a sentence, but it does not take into account the **order** of words, because the aggregation operation on top of embeddings removed this information from the original text. Because these models are unable to model word ordering, they cannot solve more complex or ambiguous tasks such as text generation or question answering. -->
åœ¨å‰é¢çš„éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬ä¸€ç›´åœ¨ä½¿ç”¨ä¸°å¯Œçš„æ–‡æœ¬è¯­ä¹‰è¡¨ç¤ºå’ŒåµŒå…¥ä¹‹ä¸Šçš„ç®€å•çº¿æ€§åˆ†ç±»å™¨ã€‚è¯¥æ¶æ„çš„ä½œç”¨æ˜¯æ•è·å¥å­ä¸­å•è¯çš„èšåˆå«ä¹‰ï¼Œä½†å®ƒæ²¡æœ‰è€ƒè™‘å•è¯çš„é¡ºåºï¼Œå› ä¸ºåµŒå…¥ä¹‹ä¸Šçš„èšåˆæ“ä½œä»åŸå§‹æ–‡æœ¬ä¸­åˆ é™¤äº†è¿™äº›ä¿¡æ¯ã€‚ç”±äºè¿™äº›æ¨¡å‹æ— æ³•å¯¹è¯åºè¿›è¡Œå»ºæ¨¡ï¼Œå› æ­¤å®ƒä»¬æ— æ³•è§£å†³æ›´å¤æ‚æˆ–æ¨¡ç³Šçš„ä»»åŠ¡ï¼Œä¾‹å¦‚æ–‡æœ¬ç”Ÿæˆæˆ–é—®ç­”ã€‚

<!-- To capture the meaning of text sequence, we need to use another neural network architecture, which is called a **recurrent neural network**, or RNN. In RNN, we pass our sentence through the network one symbol at a time, and the network produces some **state**, which we then pass to the network again with the next symbol. -->
ä¸ºäº†æ•è·æ–‡æœ¬åºåˆ—çš„å«ä¹‰ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨å¦ä¸€ç§ç¥ç»ç½‘ç»œæ¶æ„ï¼Œç§°ä¸º**å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰**ã€‚åœ¨ RNN ä¸­ï¼Œæˆ‘ä»¬ä¸€æ¬¡å°†ä¸€ä¸ªç¬¦å·é€šè¿‡ç½‘ç»œä¼ é€’å¥å­ï¼Œç½‘ç»œä¼šäº§ç”Ÿä¸€äº›**çŠ¶æ€**ï¼Œç„¶åæˆ‘ä»¬å°†å…¶ä¸ä¸‹ä¸€ä¸ªç¬¦å·å†æ¬¡ä¼ é€’åˆ°ç½‘ç»œã€‚

![RNN](./images/rnn.png)

> Image by the author

<!-- Given the input sequence of tokens X<sub>0</sub>,...,X<sub>n</sub>, RNN creates a sequence of neural network blocks, and trains this sequence end-to-end using backpropagation. Each network block takes a pair (X<sub>i</sub>,S<sub>i</sub>) as an input, and produces S<sub>i+1</sub> as a result. The final state S<sub>n</sub> or (output Y<sub>n</sub>) goes into a linear classifier to produce the result. All the network blocks share the same weights, and are trained end-to-end using one backpropagation pass. -->
ç»™å®šæ ‡è®°X<sub>0</sub>,...,X<sub>n</sub>çš„è¾“å…¥åºåˆ—ï¼ŒRNN åˆ›å»ºç¥ç»ç½‘ç»œå—åºåˆ—ï¼Œå¹¶ä½¿ç”¨åå‘ä¼ æ’­ç«¯åˆ°ç«¯åœ°è®­ç»ƒè¯¥åºåˆ—ã€‚æ¯ä¸ªç½‘ç»œå—é‡‡ç”¨ä¸€å¯¹ (X<sub>i</sub>,S<sub>i</sub>) ä½œä¸ºè¾“å…¥ï¼Œå¹¶äº§ç”ŸS<sub>i+1</sub>ä½œä¸ºç»“æœã€‚æœ€ç»ˆçŠ¶æ€S<sub>n</sub>æˆ–ï¼ˆè¾“å‡ºY<sub>n</sub>ï¼‰è¿›å…¥çº¿æ€§åˆ†ç±»å™¨ä»¥äº§ç”Ÿç»“æœã€‚æ‰€æœ‰ç½‘ç»œå—å…±äº«ç›¸åŒçš„æƒé‡ï¼Œå¹¶ä½¿ç”¨ä¸€æ¬¡åå‘ä¼ æ’­è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒã€‚

<!-- Because state vectors S<sub>0</sub>,...,S<sub>n</sub> are passed through the network, it is able to learn the sequential dependencies between words. For example, when the word *not* appears somewhere in the sequence, it can learn to negate certain elements within the state vector, resulting in negation. -->
ç”±äºçŠ¶æ€å‘é‡ S<sub>0</sub>,...,S<sub>n</sub>é€šè¿‡ç½‘ç»œä¼ é€’ï¼Œå› æ­¤èƒ½å¤Ÿå­¦ä¹ å•è¯ä¹‹é—´çš„é¡ºåºä¾èµ–å…³ç³»ã€‚ä¾‹å¦‚ï¼Œå½“å•è¯*not*å‡ºç°åœ¨åºåˆ—ä¸­çš„æŸä¸ªä½ç½®æ—¶ï¼Œå®ƒå¯ä»¥å­¦ä¹ å¯¹çŠ¶æ€å‘é‡ä¸­çš„æŸäº›å…ƒç´ æ±‚åï¼Œä»è€Œå¯¼è‡´å¦å®šã€‚

<!-- > âœ… Since the weights of all RNN blocks on the picture above are shared, the same picture can be represented as one block (on the right) with a recurrent feedback loop, which passes the output state of the network back to the input. -->

> âœ… ç”±äºä¸Šå›¾ä¸­æ‰€æœ‰ RNN å—çš„æƒé‡éƒ½æ˜¯å…±äº«çš„ï¼Œå› æ­¤åŒä¸€å¼ å›¾ç‰‡å¯ä»¥è¡¨ç¤ºä¸ºä¸€ä¸ªå…·æœ‰å¾ªç¯åé¦ˆå¾ªç¯çš„å—ï¼ˆå³ä¾§ï¼‰ï¼Œè¯¥å¾ªç¯å°†ç½‘ç»œçš„è¾“å‡ºçŠ¶æ€ä¼ é€’å›è¾“å…¥ã€‚

## Anatomy of an RNN Cell

<!-- Let's see how a simple RNN cell is organized. It accepts the previous state S<sub>i-1</sub> and current symbol X<sub>i</sub> as inputs, and has to produce the output state S<sub>i</sub> (and, sometimes, we are also interested in some other output Y<sub>i</sub>, as in the case with generative networks). -->
è®©æˆ‘ä»¬çœ‹çœ‹ä¸€ä¸ªç®€å•çš„ RNN å•å…ƒæ˜¯å¦‚ä½•ç»„ç»‡çš„ã€‚å®ƒæ¥å—å…ˆå‰çŠ¶æ€ S<sub>i-1</sub>å’Œå½“å‰ç¬¦å·X<sub>i</sub>ä½œä¸ºè¾“å…¥ï¼Œå¹¶ä¸”å¿…é¡»äº§ç”Ÿè¾“å‡ºçŠ¶æ€ S<sub>i</sub>ï¼ˆæœ‰æ—¶ï¼Œæˆ‘ä»¬ä¹Ÿå¯¹å…¶ä»–è¾“å‡º Y<sub>i</sub>æ„Ÿå…´è¶£ï¼Œå°±åƒç”Ÿæˆç½‘ç»œçš„æƒ…å†µä¸€æ ·ï¼‰ã€‚

<!-- A simple RNN cell has two weight matrices inside: one transforms an input symbol (let's call it W), and another one transforms an input state (H). In this case the output of the network is calculated as &sigma;(W&times;X<sub>i</sub>+H&times;S<sub>i-1</sub>+b), where &sigma; is the activation function and b is additional bias. -->
ä¸€ä¸ªç®€å•çš„ RNN å•å…ƒå†…éƒ¨æœ‰ä¸¤ä¸ªæƒé‡çŸ©é˜µï¼šä¸€ä¸ªå˜æ¢è¾“å…¥ç¬¦å·ï¼ˆæˆ‘ä»¬ç§°ä¹‹ä¸º Wï¼‰ï¼Œå¦ä¸€ä¸ªå˜æ¢è¾“å…¥çŠ¶æ€ (H)ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç½‘ç»œçš„è¾“å‡ºè®¡ç®—ä¸º &sigma;(W&times;X<sub>i</sub>+H&times;S<sub>i-1</sub>+b)ï¼Œå…¶ä¸­&sigma;æ˜¯æ¿€æ´»å‡½æ•°ï¼Œb æ˜¯é™„åŠ åå·®ã€‚

<img alt="RNN Cell Anatomy" src="images/rnn-anatomy.png" width="50%"/>

> Image by the author

In many cases, input tokens are passed through the embedding layer before entering the RNN to lower the dimensionality. In this case, if the dimension of the input vectors is *emb_size*, and state vector is *hid_size* - the size of W is *emb_size*&times;*hid_size*, and the size of H is *hid_size*&times;*hid_size*.
åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œè¾“å…¥æ ‡è®°åœ¨è¿›å…¥ RNN ä¹‹å‰ä¼šç»è¿‡åµŒå…¥å±‚ä»¥é™ä½ç»´åº¦ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¦‚æœè¾“å…¥å‘é‡çš„ç»´åº¦ä¸ºemb_sizeï¼ŒçŠ¶æ€å‘é‡ä¸ºhid_size - W çš„å¤§å°ä¸ºemb_size Ã— hid_sizeï¼ŒH çš„å¤§å°ä¸ºhid_size Ã— hid_sizeã€‚

## Long Short Term Memory (LSTM)

<!-- One of the main problems of classical RNNs is the so-called **vanishing gradients** problem. Because RNNs are trained end-to-end in one backpropagation pass, it has difficulty propagating error to the first layers of the network, and thus the network cannot learn relationships between distant tokens. One of the ways to avoid this problem is to introduce **explicit state management** by using so called **gates**. There are two well-known architectures of this kind: **Long Short Term Memory** (LSTM) and **Gated Relay Unit** (GRU). -->

ç»å…¸ RNN çš„ä¸»è¦é—®é¢˜ä¹‹ä¸€æ˜¯æ‰€è°“çš„**æ¢¯åº¦æ¶ˆå¤±**é—®é¢˜ã€‚ç”±äº RNN åœ¨ä¸€æ¬¡åå‘ä¼ æ’­è¿‡ç¨‹ä¸­è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒï¼Œå› æ­¤å¾ˆéš¾å°†è¯¯å·®ä¼ æ’­åˆ°ç½‘ç»œçš„ç¬¬ä¸€å±‚ï¼Œå› æ­¤ç½‘ç»œæ— æ³•å­¦ä¹ è¿œå¤„æ ‡è®°ä¹‹é—´çš„å…³ç³»ã€‚é¿å…æ­¤é—®é¢˜çš„æ–¹æ³•ä¹‹ä¸€æ˜¯é€šè¿‡ä½¿ç”¨æ‰€è°“çš„é—¨æ¥å¼•å…¥**æ˜¾å¼çŠ¶æ€ç®¡ç†**ã€‚è¿™ç§ç±»å‹æœ‰ä¸¤ç§è‘—åçš„æ¶æ„ï¼š**é•¿çŸ­æœŸè®°å¿†ï¼ˆLSTMï¼‰**å’Œ**é—¨æ§ä¸­ç»§å•å…ƒï¼ˆGRUï¼‰**ã€‚

![Image showing an example long short term memory cell](./images/long-short-term-memory-cell.svg)

> Image source TBD

<!-- The LSTM Network is organized in a manner similar to RNN, but there are two states that are being passed from layer to layer: the actual state C, and the hidden vector H. At each unit, the hidden vector H<sub>i</sub> is concatenated with input X<sub>i</sub>, and they control what happens to the state C via **gates**. Each gate is a neural network with sigmoid activation (output in the range [0,1]), which can be thought of as a bitwise mask when multiplied by the state vector. There are the following gates (from left to right on the picture above): -->
LSTM ç½‘ç»œçš„ç»„ç»‡æ–¹å¼ä¸ RNN ç±»ä¼¼ï¼Œä½†æœ‰ä¸¤ä¸ªçŠ¶æ€åœ¨å±‚ä¸å±‚ä¹‹é—´ä¼ é€’ï¼šå®é™…çŠ¶æ€ C å’Œéšè—å‘é‡ Hã€‚åœ¨æ¯ä¸ªå•å…ƒï¼Œéšè—å‘é‡H<sub>i</sub>ä¸è¾“å…¥è¿æ¥ X<sub>i</sub>ï¼Œå®ƒä»¬é€šè¿‡**gates**æ§åˆ¶çŠ¶æ€ C å‘ç”Ÿçš„æƒ…å†µã€‚æ¯ä¸ªé—¨éƒ½æ˜¯ä¸€ä¸ªå…·æœ‰ sigmoid æ¿€æ´»çš„ç¥ç»ç½‘ç»œï¼ˆè¾“å‡ºåœ¨ [0,1] èŒƒå›´å†…ï¼‰ï¼Œå½“ä¹˜ä»¥çŠ¶æ€å‘é‡æ—¶ï¼Œå¯ä»¥å°†å…¶è§†ä¸ºæŒ‰ä½æ©ç ã€‚æœ‰ä»¥ä¸‹é—¨ï¼ˆä¸Šå›¾ä»å·¦åˆ°å³ï¼‰ï¼š

<!-- * The **forget gate** takes a hidden vector and determines which components of the vector C we need to forget, and which to pass through.
* The **input gate** takes some information from the input and hidden vectors and inserts it into state.
* The **output gate** transforms state via a linear layer with *tanh* activation, then selects some of its components using a hidden vector H<sub>i</sub> to produce a new state C<sub>i+1</sub>. -->

* **forget gate**é‡‡ç”¨ä¸€ä¸ªéšè—å‘é‡å¹¶ç¡®å®šæˆ‘ä»¬éœ€è¦å¿˜è®°å‘é‡ C çš„å“ªäº›åˆ†é‡ä»¥åŠè¦é€šè¿‡å“ªäº›åˆ†é‡ã€‚
* **input gate**ä»è¾“å…¥å’Œéšè—å‘é‡ä¸­è·å–ä¸€äº›ä¿¡æ¯å¹¶å°†å…¶æ’å…¥åˆ°çŠ¶æ€ä¸­ã€‚
* **output gate**é€šè¿‡å…·æœ‰tanhæ¿€æ´»çš„çº¿æ€§å±‚è½¬æ¢çŠ¶æ€ï¼Œç„¶åä½¿ç”¨éšè—å‘é‡H<sub>i</sub>é€‰æ‹©å…¶ä¸€äº›ç»„ä»¶ä»¥äº§ç”Ÿæ–°çŠ¶æ€C<sub>i+1</sub>ã€‚

<!-- Components of the state C can be thought of as some flags that can be switched on and off. For example, when we encounter a name *Alice* in the sequence, we may want to assume that it refers to a female character, and raise the flag in the state that we have a female noun in the sentence. When we further encounter phrases *and Tom*, we will raise the flag that we have a plural noun. Thus by manipulating state we can supposedly keep track of the grammatical properties of sentence parts. -->
çŠ¶æ€Cçš„ç»„æˆéƒ¨åˆ†å¯ä»¥è¢«è®¤ä¸ºæ˜¯ä¸€äº›å¯ä»¥æ‰“å¼€å’Œå…³é—­çš„æ ‡å¿—ã€‚ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬åœ¨åºåˆ—ä¸­é‡åˆ°ä¸€ä¸ªåå­—*Alice*æ—¶ï¼Œæˆ‘ä»¬å¯èƒ½æƒ³å‡è®¾å®ƒæŒ‡çš„æ˜¯ä¸€ä¸ªå¥³æ€§è§’è‰²ï¼Œå¹¶åœ¨å¥å­ä¸­æœ‰ä¸€ä¸ªå¥³æ€§åè¯çš„æƒ…å†µä¸‹ä¸¾èµ·æ ‡å¿—ã€‚å½“æˆ‘ä»¬è¿›ä¸€æ­¥é‡åˆ°çŸ­è¯­*and Tom*æ—¶ï¼Œæˆ‘ä»¬å°†ä¸¾èµ·æˆ‘ä»¬æœ‰å¤æ•°åè¯çš„æ ‡å¿—ã€‚å› æ­¤ï¼Œé€šè¿‡æ“çºµçŠ¶æ€ï¼Œæˆ‘ä»¬å¯ä»¥è·Ÿè¸ªå¥å­éƒ¨åˆ†çš„è¯­æ³•å±æ€§ã€‚

> âœ… An excellent resource for understanding the internals of LSTM is this great article [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Christopher Olah.

## Bidirectional and Multilayer RNNs

<!-- We have discussed recurrent networks that operate in one direction, from beginning of a sequence to the end. It looks natural, because it resembles the way we read and listen to speech. However, since in many practical cases we have random access to the input sequence, it might make sense to run recurrent computation in both directions. Such networks are call **bidirectional** RNNs. When dealing with bidirectional network, we would need two hidden state vectors, one for each direction. -->

æˆ‘ä»¬å·²ç»è®¨è®ºäº†ä»åºåˆ—çš„å¼€å§‹åˆ°ç»“æŸä»¥ä¸€ä¸ªæ–¹å‘è¿è¡Œçš„å¾ªç¯ç½‘ç»œã€‚å®ƒçœ‹èµ·æ¥å¾ˆè‡ªç„¶ï¼Œå› ä¸ºå®ƒç±»ä¼¼äºæˆ‘ä»¬é˜…è¯»å’Œå¬æ¼”è®²çš„æ–¹å¼ã€‚ç„¶è€Œï¼Œç”±äºåœ¨è®¸å¤šå®é™…æƒ…å†µä¸‹æˆ‘ä»¬å¯ä»¥éšæœºè®¿é—®è¾“å…¥åºåˆ—ï¼Œå› æ­¤åœ¨ä¸¤ä¸ªæ–¹å‘ä¸Šè¿è¡Œå¾ªç¯è®¡ç®—å¯èƒ½æ˜¯æœ‰æ„ä¹‰çš„ã€‚è¿™ç§ç½‘ç»œç§°ä¸º**åŒå‘RNNs**ã€‚åœ¨å¤„ç†åŒå‘ç½‘ç»œæ—¶ï¼Œæˆ‘ä»¬éœ€è¦ä¸¤ä¸ªéšè—çŠ¶æ€å‘é‡ï¼Œæ¯ä¸ªæ–¹å‘ä¸€ä¸ªã€‚

<!-- A Recurrent network, either one-directional or bidirectional, captures certain patterns within a sequence, and can store them into a state vector or pass into output. As with convolutional networks, we can build another recurrent layer on top of the first one to capture higher level patterns and build from low-level patterns extracted by the first layer. This leads us to the notion of a **multi-layer RNN** which consists of two or more recurrent networks, where the output of the previous layer is passed to the next layer as input. -->

å•å‘æˆ–åŒå‘çš„å¾ªç¯ç½‘ç»œæ•è·åºåˆ—ä¸­çš„æŸäº›æ¨¡å¼ï¼Œå¹¶å¯ä»¥å°†å®ƒä»¬å­˜å‚¨åˆ°çŠ¶æ€å‘é‡ä¸­æˆ–ä¼ é€’åˆ°è¾“å‡ºä¸­ã€‚ä¸å·ç§¯ç½‘ç»œä¸€æ ·ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ç¬¬ä¸€ä¸ªå¾ªç¯å±‚ä¹‹ä¸Šæ„å»ºå¦ä¸€ä¸ªå¾ªç¯å±‚ï¼Œä»¥æ•è·æ›´é«˜çº§åˆ«çš„æ¨¡å¼å¹¶æ ¹æ®ç¬¬ä¸€å±‚æå–çš„ä½çº§æ¨¡å¼è¿›è¡Œæ„å»ºã€‚è¿™å¼•å‡ºäº†**å¤šå±‚ RNN**çš„æ¦‚å¿µï¼Œå®ƒç”±ä¸¤ä¸ªæˆ–å¤šä¸ªå¾ªç¯ç½‘ç»œç»„æˆï¼Œå…¶ä¸­å‰ä¸€å±‚çš„è¾“å‡ºä½œä¸ºè¾“å…¥ä¼ é€’åˆ°ä¸‹ä¸€å±‚ã€‚

![Image showing a Multilayer long-short-term-memory- RNN](./images/multi-layer-lstm.jpg)

*Picture from [this wonderful post](https://towardsdatascience.com/from-a-lstm-cell-to-a-multilayer-lstm-network-with-pytorch-2899eb5696f3) by Fernando LÃ³pez*

## âœï¸ Exercises: Embeddings

Continue your learning in the following notebooks:

* [RNNs with PyTorch](RNNPyTorch.ipynb)
* [RNNs with TensorFlow](RNNTF.ipynb)

## Conclusion

<!-- In this unit, we have seen that RNNs can be used for sequence classification, but in fact, they can handle many more tasks, such as text generation, machine translation, and more. We will consider those tasks in the next unit. -->
åœ¨æœ¬å•å…ƒä¸­ï¼Œæˆ‘ä»¬å·²ç»çœ‹åˆ° RNN å¯ç”¨äºåºåˆ—åˆ†ç±»ï¼Œä½†å®é™…ä¸Šï¼Œå®ƒä»¬å¯ä»¥å¤„ç†æ›´å¤šä»»åŠ¡ï¼Œä¾‹å¦‚æ–‡æœ¬ç”Ÿæˆã€æœºå™¨ç¿»è¯‘ç­‰ã€‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ä¸ªå•å…ƒä¸­è€ƒè™‘è¿™äº›ä»»åŠ¡ã€‚

## ğŸš€ Challenge

<!-- Read through some literature about LSTMs and consider their applications: -->
é˜…è¯»ä¸€äº›æœ‰å…³ LSTM çš„æ–‡çŒ®å¹¶è€ƒè™‘å®ƒä»¬çš„åº”ç”¨ï¼š

- [Grid Long Short-Term Memory](https://arxiv.org/pdf/1507.01526v1.pdf)
- [Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention](https://arxiv.org/pdf/1502.03044v2.pdf)

## [Post-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/216)

## Review & Self Study

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Christopher Olah.

## [Assignment: Notebooks](assignment.md)
