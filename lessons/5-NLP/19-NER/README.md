# Named Entity Recognition

<!-- Up to now, we have mostly been concentrating on one NLP task - classification. However, there are also other NLP tasks that can be accomplished with neural networks. One of those tasks is **[Named Entity Recognition](https://wikipedia.org/wiki/Named-entity_recognition)** (NER), which deals with recognizing specific entities within text, such as places, person names, date-time intervals, chemical formulae and so on. -->
åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬ä¸»è¦å…³æ³¨ä¸€é¡¹ NLP ä»»åŠ¡â€”â€”åˆ†ç±»ã€‚ç„¶è€Œï¼Œè¿˜æœ‰å…¶ä»– NLP ä»»åŠ¡å¯ä»¥é€šè¿‡ç¥ç»ç½‘ç»œæ¥å®Œæˆã€‚å…¶ä¸­ä¸€é¡¹ä»»åŠ¡æ˜¯å‘½åå®ä½“è¯†åˆ«**[Named Entity Recognition](https://wikipedia.org/wiki/Named-entity_recognition)** (NER)ï¼Œå®ƒå¤„ç†è¯†åˆ«æ–‡æœ¬ä¸­çš„ç‰¹å®šå®ä½“ï¼Œä¾‹å¦‚åœ°ç‚¹ã€äººåã€æ—¥æœŸæ—¶é—´é—´éš”ã€åŒ–å­¦å¼ç­‰ã€‚

## [Pre-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/119)

## Example of Using NER

<!-- Suppose you want to develop a natural language chat bot, similar to Amazon Alexa or Google Assistant. The way intelligent chat bots work is to *understand* what the user wants by doing text classification on the input sentence. The result of this classification is so-called **intent**, which determines what a chat bot should do. -->

å‡è®¾æ‚¨æƒ³å¼€å‘ä¸€ä¸ªè‡ªç„¶è¯­è¨€èŠå¤©æœºå™¨äººï¼Œç±»ä¼¼äº Amazon Alexa æˆ– Google Assistantã€‚æ™ºèƒ½èŠå¤©æœºå™¨äººçš„å·¥ä½œæ–¹å¼æ˜¯é€šè¿‡å¯¹è¾“å…¥å¥å­è¿›è¡Œæ–‡æœ¬åˆ†ç±»æ¥äº†è§£ç”¨æˆ·æƒ³è¦ä»€ä¹ˆã€‚è¿™ç§åˆ†ç±»çš„ç»“æœå°±æ˜¯æ‰€è°“çš„**intent**æ„å›¾ï¼Œå®ƒå†³å®šäº†èŠå¤©æœºå™¨äººåº”è¯¥åšä»€ä¹ˆã€‚

<img alt="Bot NER" src="images/bot-ner.png" width="50%"/>

> Image by the author

<!-- However, a user may provide some parameters as part of the phrase. For example, when asking for the weather, she may specify a location or date. A bot should be able to understand those entities, and fill in the parameter slots accordingly before performing the action. This is exactly where NER comes in. -->
ç„¶è€Œï¼Œç”¨æˆ·å¯ä»¥æä¾›ä¸€äº›å‚æ•°ä½œä¸ºçŸ­è¯­çš„ä¸€éƒ¨åˆ†ã€‚ä¾‹å¦‚ï¼Œå½“è¯¢é—®å¤©æ°”æ—¶ï¼Œå¥¹å¯èƒ½ä¼šæŒ‡å®šåœ°ç‚¹æˆ–æ—¥æœŸã€‚æœºå™¨äººåº”è¯¥èƒ½å¤Ÿç†è§£è¿™äº›å®ä½“ï¼Œå¹¶åœ¨æ‰§è¡Œæ“ä½œä¹‹å‰ç›¸åº”åœ°å¡«å……å‚æ•°æ§½ã€‚è¿™æ­£æ˜¯ NER å‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚

<!-- > âœ… Another example would be [analyzing scientific medical papers](https://soshnikov.com/science/analyzing-medical-papers-with-azure-and-text-analytics-for-health/). One of the main things we need to look for are specific medical terms, such as diseases and medical substances. While a small number of diseases can probably be extracted using substring search, more complex entities, such as chemical compounds and medication names, need a more complex approach. -->

> âœ… å¦ä¸€ä¸ªä¾‹å­æ˜¯åˆ†æç§‘å­¦åŒ»å­¦è®ºæ–‡[analyzing scientific medical papers](https://soshnikov.com/science/analyzing-medical-papers-with-azure-and-text-analytics-for-health/)ã€‚æˆ‘ä»¬éœ€è¦å¯»æ‰¾çš„ä¸»è¦å†…å®¹ä¹‹ä¸€æ˜¯ç‰¹å®šçš„åŒ»å­¦æœ¯è¯­ï¼Œä¾‹å¦‚ç–¾ç—…å’Œè¯ç‰©ã€‚è™½ç„¶å¯ä»¥ä½¿ç”¨å­å­—ç¬¦ä¸²æœç´¢æ¥æå–å°‘é‡ç–¾ç—…ï¼Œä½†æ›´å¤æ‚çš„å®ä½“ï¼ˆä¾‹å¦‚åŒ–åˆç‰©å’Œè¯ç‰©åç§°ï¼‰éœ€è¦æ›´å¤æ‚çš„æ–¹æ³•ã€‚

## NER as Token Classification

<!-- NER models are essentially **token classification models**, because for each of the input tokens we need to decide whether it belongs to an entity or not, and if it does - to which entity class. -->
NER æ¨¡å‹æœ¬è´¨ä¸Šæ˜¯**token classification models**token åˆ†ç±»æ¨¡å‹ï¼Œå› ä¸ºå¯¹äºæ¯ä¸ªè¾“å…¥ tokenï¼Œæˆ‘ä»¬éœ€è¦å†³å®šå®ƒæ˜¯å¦å±äºä¸€ä¸ªå®ä½“ï¼Œå¦‚æœå±äºï¼Œåˆ™å±äºå“ªä¸ªå®ä½“ç±»ã€‚

<!-- Consider the following paper title: -->
è€ƒè™‘ä»¥ä¸‹è®ºæ–‡æ ‡é¢˜ï¼š

**Tricuspid valve regurgitation** and **lithium carbonate** **toxicity** in a newborn infant.
æ–°ç”Ÿå„¿**ä¸‰å°–ç“£åæµ**å’Œ**ç¢³é…¸é”‚** **æ¯’æ€§**ã€‚

Entities here are:

* Tricuspid valve regurgitation is a disease (`DIS`)ä¸‰å°–ç“£å…³é—­ä¸å…¨æ˜¯ä¸€ç§ç–¾ç—…ï¼ˆDISï¼‰
* Lithium carbonate is a chemical substance (`CHEM`)ç¢³é…¸é”‚æ˜¯ä¸€ç§åŒ–å­¦ç‰©è´¨( CHEM)
* Toxicity is also a disease (`DIS`)ä¸­æ¯’ä¹Ÿæ˜¯ä¸€ç§ç—…ï¼ˆDISï¼‰

<!-- Notice that one entity can span several tokens. And, as in this case, we need to distinguish between two consecutive entities. Thus, it is common to use two classes for each entity - one specifying the first token of the entity (often the `B-` prefix is used, for **b**eginning), and another - the continuation of an entity (`I-`, for **i**nner token). We also use `O` as a class to represent all **o**ther tokens. Such token tagging is called [BIO tagging](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)) (or IOB). When tagged, our title will look like this: -->

è¯·æ³¨æ„ï¼Œä¸€ä¸ªå®ä½“å¯ä»¥è·¨è¶Šå¤šä¸ªä»¤ç‰Œã€‚å¹¶ä¸”ï¼Œåœ¨æœ¬ä¾‹ä¸­ï¼Œæˆ‘ä»¬éœ€è¦åŒºåˆ†ä¸¤ä¸ªè¿ç»­çš„å®ä½“ã€‚å› æ­¤ï¼Œé€šå¸¸ä¸ºæ¯ä¸ªå®ä½“ä½¿ç”¨ä¸¤ä¸ªç±» - ä¸€ä¸ªæŒ‡å®šå®ä½“çš„ç¬¬ä¸€ä¸ªæ ‡è®°ï¼ˆé€šå¸¸`B-`ä½¿ç”¨å‰ç¼€ï¼Œç”¨äºå¼€å§‹ï¼‰ï¼Œå¦ä¸€ä¸ª - å®ä½“çš„å»¶ç»­ï¼ˆ`I-`ï¼Œè¡¨ç¤ºå†…éƒ¨æ ‡è®°ï¼‰ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨`O`ä¸€ä¸ªç±»æ¥è¡¨ç¤ºæ‰€æœ‰å…¶ä»–æ ‡è®°ã€‚è¿™ç§ä»¤ç‰Œæ ‡è®°ç§°ä¸º[BIO tagging](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)æ ‡è®°ï¼ˆæˆ– IOBï¼‰ã€‚æ ‡è®°åï¼Œæˆ‘ä»¬çš„æ ‡é¢˜å°†å¦‚ä¸‹æ‰€ç¤ºï¼š

Token | Tag
------|-----
Tricuspid | B-DIS
valve | I-DIS
regurgitation | I-DIS
and | O
lithium | B-CHEM
carbonate | I-CHEM
toxicity | B-DIS
in | O
a | O
newborn | O
infant | O
. | O

<!-- Since we need to build a one-to-one correspondence between tokens and classes, we can train a rightmost **many-to-many** neural network model from this picture: -->

ç”±äºæˆ‘ä»¬éœ€è¦åœ¨æ ‡è®°å’Œç±»åˆ«ä¹‹é—´å»ºç«‹ä¸€å¯¹ä¸€çš„å¯¹åº”å…³ç³»ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ä»è¿™å¼ å›¾ä¸­è®­ç»ƒæœ€å³è¾¹çš„å¤šå¯¹å¤šç¥ç»ç½‘ç»œæ¨¡å‹ï¼š

![Image showing common recurrent neural network patterns.](../17-GenerativeNetworks/images/unreasonable-effectiveness-of-rnn.jpg)

> *Image from [this blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by [Andrej Karpathy](http://karpathy.github.io/). NER token classification models correspond to the right-most network architecture on this picture.*

## Training NER models

<!-- Since a NER model is essentially a token classification model, we can use RNNs that we are already familiar with for this task. In this case, each block of recurrent network will return the token ID. The following example notebook shows how to train LSTM for token classification. -->
ç”±äº NER æ¨¡å‹æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ª token åˆ†ç±»æ¨¡å‹ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æˆ‘ä»¬å·²ç»ç†Ÿæ‚‰çš„ RNN æ¥å®Œæˆæ­¤ä»»åŠ¡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¾ªç¯ç½‘ç»œçš„æ¯ä¸ªå—éƒ½ä¼šè¿”å›ä»£å¸ IDã€‚ä»¥ä¸‹ç¤ºä¾‹ç¬”è®°æœ¬å±•ç¤ºäº†å¦‚ä½•è®­ç»ƒ LSTM è¿›è¡Œæ ‡è®°åˆ†ç±»ã€‚

## âœï¸ Example Notebooks: NER

Continue your learning in the following notebook:

* [NER with TensorFlow](NER-TF.ipynb)

## Conclusion

<!-- A NER model is a **token classification model**, which means that it can be used to perform token classification. This is a very common task in NLP, helping to recognize specific entities within text including places, names, dates, and more. -->
NERæ¨¡å‹æ˜¯ä¸€ç§tokenåˆ†ç±»æ¨¡å‹ï¼Œè¿™æ„å‘³ç€å®ƒå¯ä»¥ç”¨æ¥è¿›è¡Œtokenåˆ†ç±»ã€‚è¿™æ˜¯ NLP ä¸­éå¸¸å¸¸è§çš„ä»»åŠ¡ï¼Œæœ‰åŠ©äºè¯†åˆ«æ–‡æœ¬ä¸­çš„ç‰¹å®šå®ä½“ï¼ŒåŒ…æ‹¬åœ°ç‚¹ã€åç§°ã€æ—¥æœŸç­‰ã€‚

## ğŸš€ Challenge

Complete the assignment linked below to train a named entity recognition model for medical terms, then try it on a different dataset.

## [Post-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/219)

## Review & Self Study

Read through the blog [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and follow along with the Further Reading section in that article to deepen your knowledge.

## [Assignment](lab/README.md)

<!-- In the assignment for this lesson, you will have to train a medical entity recognition model. You can start with training an LSTM model as described in this lesson, and proceed with using the BERT transformer model. Read [the instructions](lab/README.md) to get all the details. -->
åœ¨æœ¬è¯¾ç¨‹çš„ä½œä¸šä¸­ï¼Œæ‚¨å°†å¿…é¡»è®­ç»ƒåŒ»ç–—å®ä½“è¯†åˆ«æ¨¡å‹ã€‚æ‚¨å¯ä»¥æŒ‰ç…§æœ¬è¯¾ç¨‹ä¸­çš„æè¿°å¼€å§‹è®­ç»ƒ LSTM æ¨¡å‹ï¼Œç„¶åç»§ç»­ä½¿ç”¨ BERT è½¬æ¢å™¨æ¨¡å‹ã€‚é˜…è¯»[the instructions](lab/README.md)ä»¥è·å–æ‰€æœ‰è¯¦ç»†ä¿¡æ¯ã€‚
