<!-- # Introduction to Neural Networks: Perceptron -->
# ç¥ç»ç½‘ç»œæ¨¡å‹åŸºç¡€ï¼šæ„ŸçŸ¥æœº

## [Pre-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/103)

<!-- One of the first attempts to implement something similar to a modern neural network was done by Frank Rosenblatt from Cornell Aeronautical Laboratory in 1957. It was a hardware implementation called "Mark-1", designed to recognize primitive geometric figures, such as triangles, squares and circles. -->

åº·å¥ˆå°”èˆªç©ºå®éªŒå®¤çš„å¼—å…°å…‹Â·ç½—æ£®å¸ƒæ‹‰ç‰¹ (Frank Rosenblatt) äº 1957 å¹´é¦–æ¬¡å°è¯•å®ç°ç±»ä¼¼äºç°ä»£ç¥ç»ç½‘ç»œçš„ä¸œè¥¿ã€‚è¿™æ˜¯ä¸€ç§åä¸ºâ€œMark-1â€çš„ç¡¬ä»¶å®ç°ï¼Œæ—¨åœ¨è¯†åˆ«åŸå§‹å‡ ä½•å›¾å½¢ï¼Œä¾‹å¦‚ä¸‰è§’å½¢ã€æ­£æ–¹å½¢å’Œåœ†åœˆã€‚

|      |      |
|--------------|-----------|
|<img src='images/Rosenblatt-wikipedia.jpg' alt='Frank Rosenblatt'/> | <img src='images/Mark_I_perceptron_wikipedia.jpg' alt='The Mark 1 Perceptron' />|

> Images [from Wikipedia](https://en.wikipedia.org/wiki/Perceptron)

<!-- An input image was represented by 20x20 photocell array, so the neural network had 400 inputs and one binary output. A simple network contained one neuron, also called a **threshold logic unit**. Neural network weights acted like potentiometers that required manual adjustment during the training phase. -->

è¾“å…¥å›¾åƒç”± 20x20 å…‰ç”µç®¡é˜µåˆ—è¡¨ç¤ºï¼Œå› æ­¤ç¥ç»ç½‘ç»œæœ‰ 400 ä¸ªè¾“å…¥å’Œä¸€ä¸ªäºŒè¿›åˆ¶è¾“å‡ºã€‚ä¸€ä¸ªç®€å•çš„ç½‘ç»œåŒ…å«ä¸€ä¸ªç¥ç»å…ƒï¼Œä¹Ÿç§°ä¸ºé˜ˆå€¼é€»è¾‘å•å…ƒã€‚ç¥ç»ç½‘ç»œæƒé‡å°±åƒç”µä½å™¨ä¸€æ ·ï¼Œéœ€è¦åœ¨è®­ç»ƒé˜¶æ®µæ‰‹åŠ¨è°ƒæ•´ã€‚

<!-- > âœ… A potentiometer is a device that allows the user to adjust the resistance of a circuit. -->
> âœ… ç”µä½å™¨æ˜¯ä¸€ç§å…è®¸ç”¨æˆ·è°ƒèŠ‚ç”µè·¯ç”µé˜»çš„è£…ç½®ã€‚

<!-- > The New York Times wrote about perceptron at that time: *the embryo of an electronic computer that [the Navy] expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence.* -->

>ã€Šçº½çº¦æ—¶æŠ¥ã€‹å½“æ—¶å¯¹æ„ŸçŸ¥æœºè¿›è¡Œäº†æŠ¥é“ï¼š[æµ·å†›]æœŸæœ›ç”µå­è®¡ç®—æœºçš„èƒšèƒèƒ½å¤Ÿè¡Œèµ°ã€è¯´è¯ã€çœ‹ã€å†™ã€è‡ªæˆ‘å¤åˆ¶å¹¶æ„è¯†åˆ°è‡ªå·±çš„å­˜åœ¨ã€‚

## Perceptron Model

<!-- Suppose we have N features in our model, in which case the input vector would be a vector of size N. A perceptron is a **binary classification** model, i.e. it can distinguish between two classes of input data. We will assume that for each input vector x the output of our perceptron would be either +1 or -1, depending on the class. The output will be computed using the formula: -->

å‡è®¾æˆ‘ä»¬çš„æ¨¡å‹ä¸­æœ‰ N ä¸ªç‰¹å¾ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¾“å…¥å‘é‡å°†æ˜¯å¤§å°ä¸º N çš„å‘é‡ã€‚æ„ŸçŸ¥æœºæ˜¯äºŒå…ƒåˆ†ç±»æ¨¡å‹ï¼Œå³å®ƒå¯ä»¥åŒºåˆ†ä¸¤ç±»è¾“å…¥æ•°æ®ã€‚æˆ‘ä»¬å‡è®¾å¯¹äºæ¯ä¸ªè¾“å…¥å‘é‡ xï¼Œæ„ŸçŸ¥æœºçš„è¾“å‡ºå°†ä¸º +1 æˆ– -1ï¼Œå…·ä½“å–å†³äºç±»åˆ«ã€‚å°†ä½¿ç”¨ä»¥ä¸‹å…¬å¼è®¡ç®—è¾“å‡ºï¼š

y(x) = f(w<sup>T</sup>x)

where f is a step activation function

<!-- img src="http://www.sciweavers.org/tex2img.php?eq=f%28x%29%20%3D%20%5Cbegin%7Bcases%7D%0A%20%20%20%20%20%20%20%20%20%2B1%20%26%20x%20%5Cgeq%200%20%5C%5C%0A%20%20%20%20%20%20%20%20%20-1%20%26%20x%20%3C%200%0A%20%20%20%20%20%20%20%5Cend%7Bcases%7D%20%5C%5C%0A&bc=White&fc=Black&im=jpg&fs=12&ff=arev&edit=0" align="center" border="0" alt="f(x) = \begin{cases} +1 & x \geq 0 \\ -1 & x < 0 \end{cases} \\" width="154" height="50" / -->
<img src="images/activation-func.png"/>

## Training the Perceptron

<!-- To train a perceptron we need to find a weights vector w that classifies most of the values correctly, i.e. results in the smallest **error**. This error is defined by **perceptron criterion** in the following manner: -->

ä¸ºäº†è®­ç»ƒæ„ŸçŸ¥æœºï¼Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ°ä¸€ä¸ªæƒé‡å‘é‡ w æ¥æ­£ç¡®åˆ†ç±»å¤§å¤šæ•°å€¼ï¼Œå³äº§ç”Ÿæœ€å°çš„è¯¯å·®ã€‚è¯¥è¯¯å·®ç”±æ„ŸçŸ¥æœºæ ‡å‡†æŒ‰ä»¥ä¸‹æ–¹å¼å®šä¹‰ï¼š

E(w) = -&sum;w<sup>T</sup>x<sub>i</sub>t<sub>i</sub>

where:

<!-- * the sum is taken on those training data points i that result in the wrong classification -->
<!-- * x<sub>i</sub> is the input data, and t<sub>i</sub> is either -1 or +1 for negative and positive examples accordingly. -->

* å¯¹é‚£äº›å¯¼è‡´é”™è¯¯åˆ†ç±»çš„è®­ç»ƒæ•°æ®ç‚¹ i æ±‚å’Œ
* x<sub>i</sub>æ˜¯è¾“å…¥æ•°æ®ï¼Œå¯¹äºè´Ÿä¾‹å’Œæ­£ä¾‹ï¼Œt<sub>i</sub>ç›¸åº”åœ°ä¸º-1æˆ–+1ã€‚

<!-- This criteria is considered as a function of weights w, and we need to minimize it. Often, a method called **gradient descent** is used, in which we start with some initial weights w<sup>(0)</sup>, and then at each step update the weights according to the formula: -->

è¯¥æ ‡å‡†è¢«è§†ä¸ºæƒé‡ w çš„å‡½æ•°ï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶æœ€å°åŒ–ã€‚é€šå¸¸ï¼Œä½¿ç”¨ç§°ä¸ºæ¢¯åº¦ä¸‹é™çš„æ–¹æ³•ï¼Œå…¶ä¸­æˆ‘ä»¬ä»ä¸€äº›åˆå§‹æƒé‡w<sup>(0)</sup>å¼€å§‹ï¼Œç„¶ååœ¨æ¯ä¸€æ­¥æ ¹æ®ä»¥ä¸‹å…¬å¼æ›´æ–°æƒé‡ï¼š

w<sup>(t+1)</sup> = w<sup>(t)</sup> - &eta;&nabla;E(w)

<!-- Here &eta; is the so-called **learning rate**, and &nabla;E(w) denotes the **gradient** of E. After we calculate the gradient, we end up with -->

è¿™é‡Œ Î· å°±æ˜¯æ‰€è°“çš„å­¦ä¹ ç‡ï¼Œâˆ‡E(w) è¡¨ç¤ºE çš„æ¢¯åº¦ã€‚è®¡ç®—æ¢¯åº¦åï¼Œæˆ‘ä»¬å¾—åˆ°

w<sup>(t+1)</sup> = w<sup>(t)</sup> + &sum;&eta;x<sub>i</sub>t<sub>i</sub>

The algorithm in Python looks like this:

```python
def train(positive_examples, negative_examples, num_iterations = 100, eta = 1):

    weights = [0,0,0] # Initialize weights (almost randomly :)
        
    for i in range(num_iterations):
        pos = random.choice(positive_examples)
        neg = random.choice(negative_examples)

        z = np.dot(pos, weights) # compute perceptron output
        if z < 0: # positive example classified as negative
            weights = weights + eta*weights.shape

        z  = np.dot(neg, weights)
        if z >= 0: # negative example classified as positive
            weights = weights - eta*weights.shape

    return weights
```

## Conclusion

<!-- In this lesson, you learned about a perceptron, which is a binary classification model, and how to train it by using a weights vector. -->

åœ¨æœ¬è¯¾ç¨‹ä¸­ï¼Œæ‚¨äº†è§£äº†æ„ŸçŸ¥æœºï¼ˆä¸€ç§äºŒå…ƒåˆ†ç±»æ¨¡å‹ï¼‰ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨æƒé‡å‘é‡æ¥è®­ç»ƒå®ƒã€‚

## ğŸš€ Challenge

If you'd like to try to build your own perceptron, try [this lab on Microsoft Learn](https://docs.microsoft.com/en-us/azure/machine-learning/component-reference/two-class-averaged-perceptron?WT.mc_id=academic-77998-cacaste) which uses the [Azure ML designer](https://docs.microsoft.com/en-us/azure/machine-learning/concept-designer?WT.mc_id=academic-77998-cacaste).

## [Post-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/203)

## Review & Self Study

<!-- To see how we can use perceptron to solve a toy problem as well as real-life problems, and to continue learning - go to [Perceptron](Perceptron.ipynb) notebook. -->

<!-- Here's an interesting [article about perceptrons](https://towardsdatascience.com/what-is-a-perceptron-basics-of-neural-networks-c4cfea20c590  as well.-->

è¦äº†è§£å¦‚ä½•ä½¿ç”¨æ„ŸçŸ¥æœºæ¥è§£å†³ç©å…·é—®é¢˜å’Œç°å®ç”Ÿæ´»ä¸­çš„é—®é¢˜ï¼Œå¹¶ç»§ç»­å­¦ä¹  - è¯·è®¿é—®[æ„ŸçŸ¥æœºnotebook](Perceptron.ipynb)ã€‚

è¿™é‡Œè¿˜æœ‰ä¸€ç¯‡å…³äºæ„ŸçŸ¥æœºçš„[æœ‰è¶£æ–‡ç« ](https://towardsdatascience.com/what-is-a-perceptron-basics-of-neural-networks-c4cfea20c590
) ã€‚

## [Assignment](lab/README.md)

<!-- In this lesson, we have implemented a perceptron for binary classification task, and we have used it to classify between two handwritten digits. In this lab, you are asked to solve the problem of digit classification entirely, i.e. determine which digit is most likely to correspond to a given image. -->

åœ¨æœ¬è¯¾ä¸­ï¼Œæˆ‘ä»¬å®ç°äº†ç”¨äºäºŒå…ƒåˆ†ç±»ä»»åŠ¡çš„æ„ŸçŸ¥æœºï¼Œå¹¶ç”¨å®ƒæ¥å¯¹ä¸¤ä¸ªæ‰‹å†™æ•°å­—è¿›è¡Œåˆ†ç±»ã€‚åœ¨æœ¬å®éªŒä¸­ï¼Œæ‚¨éœ€è¦å®Œå…¨è§£å†³æ•°å­—åˆ†ç±»é—®é¢˜ï¼Œå³ç¡®å®šå“ªä¸ªæ•°å­—æœ€æœ‰å¯èƒ½å¯¹åº”äºç»™å®šå›¾åƒã€‚

* [Instructions](lab/README.md)
* [Notebook](PerceptronMultiClass.ipynb)
