# Autoencoders

<!-- When training CNNs, one of the problems is that we need a lot of labeled data. In the case of image classification, we need to separate images into different classes, which is a manual effort. -->
è®­ç»ƒ CNN æ—¶ï¼Œé—®é¢˜ä¹‹ä¸€æ˜¯æˆ‘ä»¬éœ€è¦å¤§é‡æ ‡è®°æ•°æ®ã€‚åœ¨å›¾åƒåˆ†ç±»çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éœ€è¦å°†å›¾åƒåˆ†æˆä¸åŒçš„ç±»ï¼Œè¿™æ˜¯ä¸€ä¸ªæ‰‹åŠ¨å·¥ä½œã€‚

## [Pre-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/109)

<!-- However, we might want to use raw (unlabeled) data for training CNN feature extractors, which is called **self-supervised learning**. Instead of labels, we will use training images as both network input and output. The main idea of **autoencoder** is that we will have an **encoder network** that converts input image into some **latent space** (normally it is just a vector of some smaller size), then the **decoder network**, whose goal would be to reconstruct the original image. -->
ç„¶è€Œï¼Œæˆ‘ä»¬å¯èƒ½å¸Œæœ›ä½¿ç”¨åŸå§‹ï¼ˆæœªæ ‡è®°ï¼‰æ•°æ®æ¥è®­ç»ƒ CNN ç‰¹å¾æå–å™¨ï¼Œè¿™ç§°ä¸º**è‡ªç›‘ç£å­¦ä¹ self-supervised learning**ã€‚æˆ‘ä»¬å°†ä½¿ç”¨è®­ç»ƒå›¾åƒä½œä¸ºç½‘ç»œè¾“å…¥å’Œè¾“å‡ºï¼Œè€Œä¸æ˜¯æ ‡ç­¾ã€‚**autoencoderè‡ªç¼–ç å™¨**çš„ä¸»è¦æ€æƒ³æ˜¯ï¼Œæˆ‘ä»¬å°†æœ‰ä¸€ä¸ª**encoder networkç¼–ç å™¨ç½‘ç»œ**ï¼Œå°†è¾“å…¥å›¾åƒè½¬æ¢ä¸ºæŸä¸ªæ½œåœ¨ç©ºé—´ï¼ˆé€šå¸¸å®ƒåªæ˜¯ä¸€ä¸ªè¾ƒå°å°ºå¯¸çš„å‘é‡ï¼‰ï¼Œç„¶åæ˜¯**decoder networkè§£ç å™¨ç½‘ç»œ**ï¼Œå…¶ç›®æ ‡æ˜¯é‡å»ºåŸå§‹å›¾åƒã€‚



<!-- > âœ… An [autoencoder](https://wikipedia.org/wiki/Autoencoder) is "a type of artificial neural network used to learn efficient codings of unlabeled data." -->

> âœ…è‡ªåŠ¨ç¼–ç å™¨[autoencoder](https://wikipedia.org/wiki/Autoencoder)æ˜¯â€œä¸€ç§äººå·¥ç¥ç»ç½‘ç»œï¼Œç”¨äºå­¦ä¹ æœªæ ‡è®°æ•°æ®çš„æœ‰æ•ˆç¼–ç ã€‚â€

<!-- Since we are training an autoencoder to capture as much of the information from the original image as possible for accurate reconstruction, the network tries to find the best **embedding** of input images to capture the meaning.Ğ». -->
ç”±äºæˆ‘ä»¬æ­£åœ¨è®­ç»ƒè‡ªåŠ¨ç¼–ç å™¨ä»¥ä»åŸå§‹å›¾åƒä¸­æ•è·å°½å¯èƒ½å¤šçš„ä¿¡æ¯ä»¥è¿›è¡Œå‡†ç¡®é‡å»ºï¼Œå› æ­¤ç½‘ç»œå°è¯•æ‰¾åˆ°è¾“å…¥å›¾åƒçš„æœ€ä½³**embedding**ä»¥æ•è·å«ä¹‰ã€‚

![AutoEncoder Diagram](images/autoencoder_schema.jpg)

> Image from [Keras blog](https://blog.keras.io/building-autoencoders-in-keras.html)

## Scenarios for using Autoencoders

<!-- While reconstructing original images does not seem useful in its own right, there are a few scenarios where autoencoders are especially useful: -->
è™½ç„¶é‡å»ºåŸå§‹å›¾åƒæœ¬èº«ä¼¼ä¹æ²¡ä»€ä¹ˆç”¨ï¼Œä½†åœ¨æŸäº›æƒ…å†µä¸‹è‡ªåŠ¨ç¼–ç å™¨ç‰¹åˆ«æœ‰ç”¨ï¼š

<!-- * **Lowering the dimension of images for visualization** or **training image embeddings**. Usually autoencoders give better results than PCA, because it takes into account spatial nature of images and hierarchical features.
* **Denoising**, i.e. removing noise from the image. Because noise carries out a lot of useless information, autoencoder cannot fit it all into relatively small latent space, and thus it captures only important part of the image. When training denoisers, we start with original images, and use images with artificially added noise as input for autoencoder.
* **Super-resolution**, increasing image resolution. We start with high-resolution images, and use the image with lower resolution as the autoencoder input.
* **Generative models**. Once we train the autoencoder, the decoder part can be used to create new objects starting from random latent vectors. -->

* **Lowering the dimension of images for visualization** or **training image embeddings**.ã€‚é€šå¸¸è‡ªåŠ¨ç¼–ç å™¨ç»™å‡ºçš„ç»“æœæ¯” PCA æ›´å¥½ï¼Œå› ä¸ºå®ƒè€ƒè™‘äº†å›¾åƒçš„ç©ºé—´æ€§è´¨å’Œå±‚æ¬¡ç‰¹å¾ã€‚
* **Denoisingå»å™ª**,ï¼Œå³å»é™¤å›¾åƒä¸­çš„å™ªå£°ã€‚ç”±äºå™ªå£°æºå¸¦äº†å¤§é‡æ— ç”¨ä¿¡æ¯ï¼Œè‡ªåŠ¨ç¼–ç å™¨æ— æ³•å°†å…¶å…¨éƒ¨æ”¾å…¥ç›¸å¯¹è¾ƒå°çš„æ½œåœ¨ç©ºé—´ä¸­ï¼Œå› æ­¤å®ƒä»…æ•è·å›¾åƒçš„é‡è¦éƒ¨åˆ†ã€‚åœ¨è®­ç»ƒé™å™ªå™¨æ—¶ï¼Œæˆ‘ä»¬ä»åŸå§‹å›¾åƒå¼€å§‹ï¼Œå¹¶ä½¿ç”¨äººå·¥æ·»åŠ å™ªå£°çš„å›¾åƒä½œä¸ºè‡ªåŠ¨ç¼–ç å™¨çš„è¾“å…¥ã€‚
* **Super-resolution**ï¼Œæé«˜å›¾åƒåˆ†è¾¨ç‡ã€‚æˆ‘ä»¬ä»é«˜åˆ†è¾¨ç‡å›¾åƒå¼€å§‹ï¼Œå¹¶ä½¿ç”¨è¾ƒä½åˆ†è¾¨ç‡çš„å›¾åƒä½œä¸ºè‡ªåŠ¨ç¼–ç å™¨è¾“å…¥ã€‚
* **Generative models**ã€‚ä¸€æ—¦æˆ‘ä»¬è®­ç»ƒäº†è‡ªåŠ¨ç¼–ç å™¨ï¼Œè§£ç å™¨éƒ¨åˆ†å°±å¯ä»¥ç”¨äºä»éšæœºæ½œåœ¨å‘é‡å¼€å§‹åˆ›å»ºæ–°å¯¹è±¡ã€‚

## Variational Autoencoders (VAE)

<!-- Traditional autoencoders reduce the dimension of the input data somehow, figuring out the important features of input images. However, latent vectors ofter do not make much sense. In other words, taking MNIST dataset as an example, figuring out which digits correspond to different latent vectors is not an easy task, because close latent vectors would not necessarily correspond to the same digits. -->
ä¼ ç»Ÿçš„è‡ªåŠ¨ç¼–ç å™¨ä»¥æŸç§æ–¹å¼å‡å°‘è¾“å…¥æ•°æ®çš„ç»´åº¦ï¼Œæ‰¾å‡ºè¾“å…¥å›¾åƒçš„é‡è¦ç‰¹å¾ã€‚ç„¶è€Œï¼Œæ½œåœ¨å‘é‡é€šå¸¸æ²¡æœ‰å¤šå¤§æ„ä¹‰ã€‚æ¢å¥è¯è¯´ï¼Œä»¥ MNIST æ•°æ®é›†ä¸ºä¾‹ï¼Œæ‰¾å‡ºå“ªäº›æ•°å­—å¯¹åº”äºä¸åŒçš„æ½œåœ¨å‘é‡å¹¶ä¸æ˜¯ä¸€ä»¶å®¹æ˜“çš„ä»»åŠ¡ï¼Œå› ä¸ºæ¥è¿‘çš„æ½œåœ¨å‘é‡ä¸ä¸€å®šå¯¹åº”äºç›¸åŒçš„æ•°å­—ã€‚

<!-- On the other hand, to train *generative* models it is better to have some understanding of the latent space. This idea leads us to **variational auto-encoder** (VAE). -->
å¦ä¸€æ–¹é¢ï¼Œä¸ºäº†è®­ç»ƒç”Ÿæˆæ¨¡å‹ï¼Œæœ€å¥½å¯¹æ½œåœ¨ç©ºé—´æœ‰ä¸€äº›äº†è§£ã€‚è¿™ä¸ªæƒ³æ³•è®©æˆ‘ä»¬æƒ³åˆ°äº†å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨**variational auto-encoder** (VAE)ã€‚

<!-- VAE is the autoencoder that learns to predict *statistical distribution* of the latent parameters, so-called **latent distribution**. For example, we may want latent vectors to be distributed normally with some mean z<sub>mean</sub> and standard deviation z<sub>sigma</sub> (both mean and standard deviation are vectors of some dimensionality d). Encoder in VAE learns to predict those parameters, and then decoder takes a random vector from this distribution to reconstruct the object. -->
VAE æ˜¯å­¦ä¹ é¢„æµ‹æ½œåœ¨å‚æ•°çš„ç»Ÿè®¡åˆ†å¸ƒï¼ˆå³æ‰€è°“çš„æ½œåœ¨åˆ†å¸ƒ**latent distribution**ï¼‰çš„è‡ªåŠ¨ç¼–ç å™¨ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯èƒ½å¸Œæœ›æ½œåœ¨å‘é‡å‘ˆæ­£æ€åˆ†å¸ƒï¼Œå…·æœ‰æŸä¸ªå¹³å‡å€¼ z<sub>mean</sub>å’Œæ ‡å‡†å·® z<sub>sigma</sub>ï¼ˆå¹³å‡å€¼å’Œæ ‡å‡†å·®éƒ½æ˜¯æŸä¸ªç»´åº¦ d çš„å‘é‡ï¼‰ã€‚VAE ä¸­çš„ç¼–ç å™¨å­¦ä¹ é¢„æµ‹è¿™äº›å‚æ•°ï¼Œç„¶åè§£ç å™¨ä»è¯¥åˆ†å¸ƒä¸­è·å–éšæœºå‘é‡æ¥é‡å»ºå¯¹è±¡ã€‚

To summarize:

 * From input vector, we predict `z_mean` and `z_log_sigma` (instead of predicting the standard deviation itself, we predict its logarithm)
 * We sample a vector `sample` from the distribution N(z<sub>mean</sub>,exp(z<sub>log\_sigma</sub>))
 * The decoder tries to decode the original image using `sample` as an input vector

 <img src="images/vae.png" width="50%">

> Image from [this blog post](https://ijdykeman.github.io/ml/2016/12/21/cvae.html) by Isaak Dykeman

<!-- Variational auto-encoders use a complex loss function that consists of two parts: -->
å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ä½¿ç”¨å¤æ‚çš„æŸå¤±å‡½æ•°ï¼Œè¯¥å‡½æ•°ç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼š

<!-- * **Reconstruction loss** is the loss function that shows how close a reconstructed image is to the target (it can be Mean Squared Error, or MSE). It is the same loss function as in normal autoencoders.
* **KL loss**, which ensures that latent variable distributions stays close to normal distribution. It is based on the notion of [Kullback-Leibler divergence](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained) - a metric to estimate how similar two statistical distributions are. -->

* **Reconstruction lossé‡å»ºæŸå¤±**æ˜¯æ˜¾ç¤ºé‡å»ºå›¾åƒä¸ç›®æ ‡çš„æ¥è¿‘ç¨‹åº¦çš„æŸå¤±å‡½æ•°ï¼ˆå¯ä»¥æ˜¯å‡æ–¹è¯¯å·®ï¼Œæˆ– MSEï¼‰ã€‚å®ƒä¸æ™®é€šè‡ªåŠ¨ç¼–ç å™¨ä¸­çš„æŸå¤±å‡½æ•°ç›¸åŒã€‚
* **KL lossï¼ŒKL æŸå¤±**ï¼Œç¡®ä¿æ½œåœ¨å˜é‡åˆ†å¸ƒä¿æŒæ¥è¿‘æ­£æ€åˆ†å¸ƒã€‚å®ƒåŸºäº[Kullback-Leibler divergence](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained)æ•£åº¦çš„æ¦‚å¿µ- ä¸€ç§ä¼°è®¡ä¸¤ä¸ªç»Ÿè®¡åˆ†å¸ƒç›¸ä¼¼ç¨‹åº¦çš„æŒ‡æ ‡ã€‚

<!-- One important advantage of VAEs is that they allow us to generate new images relatively easily, because we know which distribution from which to sample latent vectors. For example, if we train VAE with 2D latent vector on MNIST, we can then vary components of the latent vector to get different digits: -->
VAE çš„ä¸€ä¸ªé‡è¦ä¼˜ç‚¹æ˜¯å®ƒä»¬ä½¿æˆ‘ä»¬èƒ½å¤Ÿç›¸å¯¹è½»æ¾åœ°ç”Ÿæˆæ–°å›¾åƒï¼Œå› ä¸ºæˆ‘ä»¬çŸ¥é“ä»å“ªä¸ªåˆ†å¸ƒä¸­å¯¹æ½œåœ¨å‘é‡è¿›è¡Œé‡‡æ ·ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬åœ¨ MNIST ä¸Šä½¿ç”¨ 2D æ½œåœ¨å‘é‡è®­ç»ƒ VAEï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥æ”¹å˜æ½œåœ¨å‘é‡çš„ç»„æˆéƒ¨åˆ†ä»¥è·å¾—ä¸åŒçš„æ•°å­—ï¼š

<img alt="vaemnist" src="images/vaemnist.png" width="50%"/>

> Image by [Dmitry Soshnikov](http://soshnikov.com)

<!-- Observe how images blend into each other, as we start getting latent vectors from the different portions of the latent parameter space. We can also visualize this space in 2D: -->
å½“æˆ‘ä»¬å¼€å§‹ä»æ½œåœ¨å‚æ•°ç©ºé—´çš„ä¸åŒéƒ¨åˆ†è·å–æ½œåœ¨å‘é‡æ—¶ï¼Œè§‚å¯Ÿå›¾åƒå¦‚ä½•ç›¸äº’æ··åˆã€‚æˆ‘ä»¬è¿˜å¯ä»¥ä»¥äºŒç»´æ–¹å¼å¯è§†åŒ–è¿™ä¸ªç©ºé—´ï¼š



<img alt="vaemnist cluster" src="images/vaemnist-diag.png" width="50%"/> 

> Image by [Dmitry Soshnikov](http://soshnikov.com)

## âœï¸ Exercises: Autoencoders

Learn more about autoencoders in these corresponding notebooks:

* [Autoencoders in TensorFlow](AutoencodersTF.ipynb)
* [Autoencoders in PyTorch](AutoEncodersPyTorch.ipynb)

## Properties of Autoencoders

<!-- * **Data Specific** - they only work well with the type of images they have been trained on. For example, if we train a super-resolution network on flowers, it will not work well on portraits. This is because the network can produce higher resolution image by taking fine details from features learned from the training dataset.
* **Lossy** - the reconstructed image is not the same as the original image. The nature of loss is defined by the *loss function* used during training
* Works on **unlabeled data** -->

## [Post-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/209)

* **Data Specificæ•°æ®ç‰¹å®š**- å®ƒä»¬ä»…é€‚ç”¨äºç»è¿‡è®­ç»ƒçš„å›¾åƒç±»å‹ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬åœ¨èŠ±æœµä¸Šè®­ç»ƒè¶…åˆ†è¾¨ç‡ç½‘ç»œï¼Œåˆ™å®ƒåœ¨è‚–åƒä¸Šæ•ˆæœä¸ä½³ã€‚è¿™æ˜¯å› ä¸ºç½‘ç»œå¯ä»¥é€šè¿‡ä»è®­ç»ƒæ•°æ®é›†ä¸­å­¦ä¹ åˆ°çš„ç‰¹å¾è·å–ç²¾ç»†ç»†èŠ‚æ¥ç”Ÿæˆæ›´é«˜åˆ†è¾¨ç‡çš„å›¾åƒã€‚
* **Lossyæœ‰æŸ**- é‡å»ºå›¾åƒä¸åŸå§‹å›¾åƒä¸åŒã€‚æŸå¤±çš„æ€§è´¨ç”±è®­ç»ƒæœŸé—´ä½¿ç”¨çš„æŸå¤±å‡½æ•°å®šä¹‰
* é€‚ç”¨äº**unlabeled dataæœªæ ‡è®°çš„æ•°æ®**

## Conclusion

<!-- In this lesson, you learned about the various types of autoencoders available to the AI scientist. You learned how to build them, and how to use them to reconstruct images. You also learned about the VAE and how to use it to generate new images. -->
åœ¨æœ¬è¯¾ç¨‹ä¸­ï¼Œæ‚¨äº†è§£äº†äººå·¥æ™ºèƒ½ç§‘å­¦å®¶å¯ç”¨çš„å„ç§ç±»å‹çš„è‡ªåŠ¨ç¼–ç å™¨ã€‚æ‚¨å­¦ä¹ äº†å¦‚ä½•æ„å»ºå®ƒä»¬ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨å®ƒä»¬æ¥é‡å»ºå›¾åƒã€‚æ‚¨è¿˜äº†è§£äº† VAE ä»¥åŠå¦‚ä½•ä½¿ç”¨å®ƒç”Ÿæˆæ–°å›¾åƒã€‚

## ğŸš€ Challenge

In this lesson, you learned about using autoencoders for images. But they can also be used for music! Check out the Magenta project's [MusicVAE](https://magenta.tensorflow.org/music-vae) project, which uses autoencoders to learn to reconstruct music. Do some [experiments](https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/Multitrack_MusicVAE.ipynb) with this library to see what you can create.

## [Post-lecture quiz](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/208)

## Review & Self Study

For reference, read more about autoencoders in these resources:

* [Building Autoencoders in Keras](https://blog.keras.io/building-autoencoders-in-keras.html)
* [Blog post on NeuroHive](https://neurohive.io/ru/osnovy-data-science/variacionnyj-avtojenkoder-vae/)
* [Variational Autoencoders Explained](https://kvfrans.com/variational-autoencoders-explained/)
* [Conditional Variational Autoencoders](https://ijdykeman.github.io/ml/2016/12/21/cvae.html)

## Assignment

At the end of [this notebook using TensorFlow](AutoencodersTF.ipynb), you will find a 'task' - use this as your assignment.